{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Homework: Evaluation and Monitoring\n",
    "\n",
    "import pandas as pd\n",
    "url = f'{'https://github.com/varunsood189/llm-zoomcap/blob/main/lesson4/data/results-jamba_1_5_mini.csv'}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "df = df.iloc[:300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "## Q1. Getting the embeddings model\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.45160076)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n",
    "embedding_model.encode(answer_llm)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What's the first value of the resulting vector?\n",
    "* -0.45\n",
    "## Q2. Computing the dot product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'Yes. You can sign up for the course on the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials.',\n",
       " 'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       " 'document': '0227b872',\n",
       " 'question': 'Where can I sign up for the course?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:17<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "evaluations =[]\n",
    "for data in tqdm(records):\n",
    "    # print(data[\"answer_llm\"])\n",
    "    emb_llm=embedding_model.encode(data[\"answer_llm\"])\n",
    "    emb_orig=embedding_model.encode(data[\"answer_orig\"])\n",
    "    evaluations+=[ np.dot(emb_llm,emb_orig)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(30.403513)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(evaluations ,75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the 75% percentile of the score?\n",
    "* 31.67\n",
    "\n",
    "## Q3. Computing the cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:17<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "def norm_v(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    return v / norm\n",
    "    \n",
    "evaluations_norm =[]\n",
    "for data in tqdm(records):\n",
    "    # print(data[\"answer_llm\"])\n",
    "    emb_llm=embedding_model.encode(data[\"answer_llm\"])\n",
    "    emb_orig=embedding_model.encode(data[\"answer_orig\"])\n",
    "    evaluations_norm+=[ np.dot(norm_v(emb_llm),norm_v(emb_orig))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.80615664)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(evaluations_norm,75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the 75% cosine in the scores?\n",
    "np.float32(0.80615664)\n",
    "\n",
    "## Q4. Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.12/site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8923076873088758"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "r = records[10]\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "scores['rouge-1']['f']\n",
    "# * `rouge-1` - the overlap of unigrams,\n",
    "# * `rouge-2` - bigrams,\n",
    "# * `rouge-l` - the longest common subsequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the F score for `rouge-1`?\n",
    "0.89\n",
    "\n",
    "## Q5. Average rouge score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8704273454284358\n"
     ]
    }
   ],
   "source": [
    "avg_f = []\n",
    "for key in scores.keys():\n",
    "    avg_f+=[scores[key]['f']]\n",
    "print(sum(avg_f)/len(avg_f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the average F-score between `rouge-1`, `rouge-2` and `rouge-l` for the same record from Q4\n",
    "* 0.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 512.97it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "for r in tqdm(records):\n",
    "    rouge =rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "    evaluations+=[\n",
    "        {'rouge-1':rouge['rouge-1']['f'],\n",
    "        'rouge-2':rouge['rouge-2']['f'],\n",
    "        'rouge-l':rouge['rouge-l']['f'],\n",
    "        'avergae-rouge':(rouge['rouge-1']['f']+rouge['rouge-2']['f']+rouge['rouge-l']['f'])/2,\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.19068844277809807)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rouge-2'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the average F-score in `rouge_2` across all the records?\n",
    "- 0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
